# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/126STlkXSUESU-fpyS1BZvrxxfhMwUS8d
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df_train = pd.read_csv("/content/Titanic_train.csv")

# Exploratory Data Analysis
print("## Data Exploration")
df_train.head()

df_train.info()

df_train.describe()

# Visualizations
sns.histplot(df_train['Age'].dropna(), bins=30, kde=True)
plt.title("Age Distribution")
plt.show

sns.boxplot(x=df_train['Pclass'], y=df_train['Fare'])
plt.title("Fare Distribution by Class")
plt.show()

sns.pairplot(df_train[['Age', 'Fare', 'SibSp', 'Parch', 'Survived']])
plt.show()

# Correlation Heatmap
import seaborn as sns
import matplotlib.pyplot as plt
numeric_df = df_train.select_dtypes(include=['number'])

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# Handling missing values
imputer = SimpleImputer(strategy='mean')
df_train['Age'] = imputer.fit_transform(df_train[['Age']])
df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)
df_train.drop(columns=['Cabin'], inplace=True)

# Encoding categorical variables
label_encoder = LabelEncoder()
df_train['Sex'] = label_encoder.fit_transform(df_train['Sex'])
df_train['Embarked'] = label_encoder.fit_transform(df_train['Embarked'])

print("Data preprocessing completed.")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Selecting features and target
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
X = df_train[features]
y = df_train['Survived']

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Building
model = LogisticRegression()
model.fit(X_train, y_train)

print("Model training completed.")

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Model Evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred))

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
plt.figure()
plt.plot(fpr, tpr, label="ROC curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# Testing Dataset / Model Evaluation
titanic_test = pd.read_csv('/content/Titanic_test.csv')

titanic_test.head()

titanic_test.info()

titanic_test.describe()

titanic_test.isnull().sum()

titanic_test["Embarked"].value_counts()

titanic_test["Sex"].value_counts()

titanic_test['Age'].fillna(titanic_test['Age'].median(), inplace=True)

titanic_test.drop(columns=["PassengerId", "Cabin", "Ticket", "Name"], inplace = True)

titanic_test.head()

titanic_test = pd.get_dummies(titanic_test, columns=['Sex', 'Embarked'], drop_first=True)

titanic_test.head()

titanic_test.info()

titanic_test['Fare'].fillna(titanic_test['Fare'].median(), inplace = True)

def preprocess_data(df):
    # Handling missing values
    imputer = SimpleImputer(strategy='mean')
    df['Age'] = imputer.fit_transform(df[['Age']])
    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
    df['Fare'].fillna(df['Fare'].median(), inplace=True)  # Fill missing Fare

    df.drop(columns=['Cabin', 'Ticket', 'Name', 'PassengerId'], inplace=True, errors='ignore')


    label_encoder = LabelEncoder()
    df['Sex'] = label_encoder.fit_transform(df['Sex'])
    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])

    return df

df_train = pd.read_csv("/content/Titanic_train.csv")
titanic_test = pd.read_csv('/content/Titanic_test.csv')

df_train = preprocess_data(df_train)
titanic_test = preprocess_data(titanic_test)


features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
X = df_train[features]
y = df_train['Survived']


# Prediction on the test set
predictions = model.predict(titanic_test[features])

model.predict(titanic_test)

pd.Series(model.predict(titanic_test))

titanic_test['Servived'] = pd.Series(model.predict(titanic_test))

titanic_test.head()

titanic_test.head()

"""## Streamlit App Deployment"""

!pip install streamlit

import streamlit as st

def main():
    st.title("Titanic Survival Prediction")
    st.write("Enter passenger details to predict survival.")

    # User inputs
    pclass = st.selectbox("Passenger Class", [1, 2, 3])
    sex = st.radio("Sex", ["Male", "Female"])
    age = st.slider("Age", 1, 100, 25)
    sibsp = st.number_input("Siblings/Spouses Aboard", 0, 10, 0)
    parch = st.number_input("Parents/Children Aboard", 0, 10, 0)
    fare = st.number_input("Fare", 0.0, 500.0, 50.0)
    embarked = st.selectbox("Embarked Port", ["C", "Q", "S"])

if st.button("Predict Survival"):
        user_data = pd.DataFrame([[pclass, 1 if sex == "Male" else 0, age, sibsp, parch, fare,
                                   1 if embarked == "Q" else 0, 1 if embarked == "S" else 0]],
                                 columns=X.columns)
        user_data = scaler.transform(user_data)
        prediction = model.predict(user_data)
        st.write("Survived" if prediction[0] == 1 else "Did Not Survive")

if __name__ == "__main__":
    main()

"""# Interview Questions:

"""

##1. What is the difference between precision and recall?

Precision and recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where the classes are imbalanced.

1. **Precision**: This measures the accuracy of the positive predictions made by the model. Specifically, it is the ratio of true positive predictions to the total number of positive predictions (both true positives and false positives). In formula terms:

   \[
   \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
   \]

   Precision tells us how many of the predicted positive instances are actually positive. High precision means that when the model predicts a positive outcome, it is likely correct.

2. **Recall**: Also known as sensitivity or true positive rate, recall measures how well the model identifies all the actual positive instances. It is the ratio of true positive predictions to the total number of actual positives (both true positives and false negatives). In formula terms:

   \[
   \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
   \]

   Recall tells us how many of the actual positive instances were correctly identified by the model. High recall means that the model is good at capturing most of the positive cases.

**In summary:**

- **Precision** is about the quality of positive predictions. It answers the question: "Of all the positive predictions, how many were actually positive?"
- **Recall** is about the completeness of positive predictions. It answers the question: "Of all the actual positives, how many were identified by the model?"

A model with high precision and high recall is ideal, but in practice, there is often a trade-off between the two. For example, increasing precision typically decreases recall and vice versa.

## 2.What is cross-validation, and why is it important in binary classification?

Cross-validation is a technique used in statistical modeling and machine learning to assess how well a model generalizes to an independent dataset. It involves partitioning the dataset into multiple subsets or folds, training the model on some of these folds, and evaluating it on the remaining folds. This process helps in obtaining a more reliable estimate of the model’s performance compared to using a single train-test split.

**Here's how cross-validation generally works:**

1. **Partitioning the Data**: The dataset is divided into \( k \) subsets or folds (commonly 5 or 10).

2. **Training and Testing**: For each fold:
   - The model is trained on \( k-1 \) folds (the training set).
   - The remaining fold is used as the test set to evaluate the model’s performance.

3. **Repeating**: This process is repeated \( k \) times, with each fold being used as the test set exactly once.

4. **Aggregating Results**: The performance metrics (such as accuracy, precision, recall, etc.) are averaged over all \( k \) folds to provide a final estimate of the model’s performance.

**Importance of Cross-Validation in Binary Classification:**

1. **More Reliable Performance Estimates**: By evaluating the model on multiple subsets of data, cross-validation provides a more robust estimate of how the model will perform on unseen data. It reduces the risk of overfitting to a particular train-test split.

2. **Efficient Use of Data**: In cases where the dataset is small, cross-validation helps to make the most of the available data by using each data point for both training and testing. This can lead to better utilization of the dataset and a more reliable assessment.

3. **Model Selection and Tuning**: Cross-validation helps in selecting the best model and hyperparameters by providing a way to compare different models or configurations based on their average performance across the folds. It helps in understanding how changes in model parameters impact performance.

4. **Detecting Variability**: Cross-validation helps in understanding the variability in model performance across different subsets of the data. This can reveal whether the model's performance is consistent or if it varies significantly based on the specific data it was trained and tested on.

5. **Avoiding Data Leakage**: Proper cross-validation ensures that the test set is independent of the training data, which helps in avoiding data leakage and provides a more accurate measure of model performance.

**Common Types of Cross-Validation:**

- **K-Fold Cross-Validation**: As described, where the dataset is divided into \( k \) folds.
- **Leave-One-Out Cross-Validation (LOOCV)**: A special case of \( k \)-fold cross-validation where \( k \) equals the number of data points, meaning each fold consists of a single data point as the test set.
- **Stratified K-Fold Cross-Validation**: A variation where the folds are made to maintain the proportion of each class, which is particularly useful for imbalanced datasets.

In binary classification, cross-validation helps ensure that the model's performance metrics (like precision, recall, and F1 score) are reliable and not just artifacts of a particular train-test split.